{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import joblib\n",
    "import quaternion as q\n",
    "import numpy as np\n",
    "from src.models.autoencoder.autoenc import Embedder\n",
    "from src.utils.render_utils import add_agent_view_on_w, add_title\n",
    "import imageio\n",
    "from IPython.display import HTML, display\n",
    "import copy\n",
    "from src.utils.camera_trajectory import go_forward, go_backward, rotate_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-09 13:39:23,114 Initializing dataset PointNav-v1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['GLOG_minloglevel'] = \"3\"\n",
    "os.environ['MAGNUM_LOG'] = \"quiet\"\n",
    "os.environ['HABITAT_SIM_LOG'] = \"quiet\"\n",
    "import habitat\n",
    "from habitat import get_config\n",
    "from habitat.sims import make_sim\n",
    "\n",
    "config = get_config()\n",
    "cfg = config\n",
    "cfg.defrost()\n",
    "habitat_api_path = os.path.join(os.path.dirname(habitat.__file__), '../')\n",
    "cfg.DATASET.SCENES_DIR = os.path.join(habitat_api_path, cfg.DATASET.SCENES_DIR)\n",
    "cfg.DATASET.DATA_PATH = os.path.join(habitat_api_path, cfg.DATASET.DATA_PATH.replace(\"habitat-test-scenes\",\"gibson\"))\n",
    "cfg.SIMULATOR.SCENE_DATASET = 'data/scene_datasets/mp3d.scene_dataset_config.json'\n",
    "cfg.SIMULATOR.SCENE_DATASET = os.path.join(habitat_api_path, cfg.SIMULATOR.SCENE_DATASET )\n",
    "cfg.DATASET.TYPE = \"PointNav-v1\"\n",
    "cfg.SIMULATOR.RGB_SENSOR.HEIGHT = 128\n",
    "cfg.SIMULATOR.RGB_SENSOR.WIDTH = 128\n",
    "cfg.SIMULATOR.DEPTH_SENSOR.HEIGHT = 128\n",
    "cfg.SIMULATOR.DEPTH_SENSOR.WIDTH = 128\n",
    "cfg.TASK.SENSORS = cfg.SIMULATOR.AGENT_0.SENSORS = ['RGB_SENSOR', 'DEPTH_SENSOR']\n",
    "cfg.SIMULATOR.AGENT_0.HEIGHT = 1.25\n",
    "cfg.freeze()\n",
    "\n",
    "from habitat.datasets import make_dataset\n",
    "dataset = make_dataset('PointNav-v1')\n",
    "cfg.defrost()\n",
    "cfg.DATASET.TYPE = 'PointNav-v1'\n",
    "cfg.DATASET.SPLIT = 'val'\n",
    "cfg.freeze()\n",
    "all_scenes = dataset.get_scenes_to_load(cfg.DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "embedder = Embedder(pretrained_ckpt='pretrained/autoenc_large.ckpt',\n",
    "                   img_res=128, w_size=128, coordinate_scale=32, w_ch=32, nerf_res=128, voxel_res=128)\n",
    "embedder = embedder.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantwell\n"
     ]
    }
   ],
   "source": [
    "scene = 'Cantwell'#np.random.choice(all_scenes)\n",
    "print(scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-09 13:39:27,006 initializing sim Sim-v0\n"
     ]
    }
   ],
   "source": [
    "try: sim.close()\n",
    "except: pass\n",
    "cfg.defrost()\n",
    "cfg.SIMULATOR.SCENE = os.path.join(cfg.DATASET.SCENES_DIR,'gibson_habitat/{}.glb'.format(scene))\n",
    "cfg.freeze()\n",
    "past_room = scene\n",
    "sim = make_sim(id_sim=cfg.SIMULATOR.TYPE, config=cfg.SIMULATOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Embed Random Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GOALS = 5\n",
    "from habitat.tasks.nav.shortest_path_follower import ShortestPathFollower\n",
    "\n",
    "follower = ShortestPathFollower(sim, 0.5, return_one_hot=False)\n",
    "\n",
    "start_position = sim.sample_navigable_point()\n",
    "start_rotation = q.from_euler_angles([0., np.random.rand() * 2 * np.pi, 0.])\n",
    "\n",
    "orig_Rt = np.eye(4)\n",
    "orig_Rt[:3,3] = start_position\n",
    "orig_Rt[:3,:3] = q.as_rotation_matrix(start_rotation)\n",
    "orig_Rt = np.linalg.inv(orig_Rt)\n",
    "\n",
    "goal_points = []\n",
    "for i in range(NUM_GOALS):\n",
    "    while True:\n",
    "        random_point = sim.sample_navigable_point()\n",
    "        if abs(random_point[1]-start_position[1]) > 0.5: continue\n",
    "        break\n",
    "    goal_points.append(random_point)\n",
    "\n",
    "sim.set_agent_state(start_position, start_rotation)\n",
    "done = False\n",
    "goal_point = goal_points.pop(0)\n",
    "\n",
    "K = torch.eye(3)\n",
    "K[0,0] = (embedder.img_res/2.) / np.tan(np.deg2rad(90.0) / 2)\n",
    "K[1,1] = -(embedder.img_res/2.) / np.tan(np.deg2rad(90.0) / 2)\n",
    "K = K.unsqueeze(0).to(device)\n",
    "\n",
    "w, w_mask = None, None\n",
    "imgs = []\n",
    "while not done:\n",
    "    state = sim.get_agent_state()\n",
    "    obs = sim.get_observations_at(state.position, state.rotation)\n",
    "\n",
    "    Rt_t = np.eye(4)\n",
    "    Rt_t[:3,3] = state.position\n",
    "    Rt_t[:3,:3] = q.as_rotation_matrix(state.rotation)\n",
    "    Rt_t = np.linalg.inv(Rt_t)\n",
    "    Rt_t = Rt_t @ np.linalg.inv(orig_Rt)\n",
    "\n",
    "    rgb_t = torch.from_numpy(obs['rgb']/255.).unsqueeze(0).permute(0,3,1,2).to(device)\n",
    "    depth_t = torch.from_numpy(obs['depth']).unsqueeze(0).permute(0,3,1,2).to(device)\n",
    "    Rt_t = torch.from_numpy(Rt_t).unsqueeze(0).float().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = embedder.calculate_mask_func(depth_t*10.0, Rt_t, K)\n",
    "        sorted_indices, seq_unique_list, seq_unique_counts, _ = output\n",
    "        input_dict = {'rgb': rgb_t.unsqueeze(1),\n",
    "                      'depth': depth_t.unsqueeze(1),\n",
    "                    'sorted_indices': sorted_indices.unsqueeze(1),\n",
    "                    'seq_unique_counts': seq_unique_counts.unsqueeze(1),\n",
    "                      'seq_unique_list': seq_unique_list.unsqueeze(1)}\n",
    "        w, w_mask = embedder.embed_obs(input_dict, past_w=w, past_w_num_mask=w_mask)\n",
    "        recon_rgb, _ = embedder.generate(w, {'Rt': Rt_t.unsqueeze(1), 'K':K.unsqueeze(1)}, out_res=64)\n",
    "\n",
    "        orig_rgb = add_title(obs['rgb'], 'observation')\n",
    "        recon_rgb = (recon_rgb.squeeze().permute(1,2,0).detach().cpu() * 255).numpy().astype(np.uint8)\n",
    "        recon_rgb = cv2.resize(recon_rgb, (orig_rgb.shape[0], orig_rgb.shape[1]))\n",
    "        recon_rgb = add_title(recon_rgb, 'recon obs.')\n",
    "\n",
    "        w_im = w.mean(0).mean(0).detach().cpu().numpy()\n",
    "        w_im = ((w_im - w_im.min())/(w_im.max()-w_im.min()) * 255).astype(np.uint8)\n",
    "        w_im = cv2.applyColorMap(w_im, cv2.COLORMAP_VIRIDIS)[:,:,::-1]\n",
    "        last_w_im = w_im\n",
    "        w_im = add_agent_view_on_w(w_im, Rt_t, embedder.coordinate_scale, embedder.w_size, agent_size=4, view_size=15)\n",
    "        w_im = np.fliplr(w_im)\n",
    "        w_im = add_title(w_im, 'RNR-map')\n",
    "\n",
    "    view_im = np.concatenate([orig_rgb, recon_rgb, w_im],1)\n",
    "    imgs.append(view_im)\n",
    "\n",
    "    cv2.imshow(\"view\", view_im[:,:,::-1])\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord(\"q\"): break\n",
    "\n",
    "\n",
    "\n",
    "    action = follower.get_next_action(goal_point)\n",
    "    if action is None or action == 0:\n",
    "        if len(goal_points) == 0:\n",
    "            break\n",
    "        goal_point = goal_points.pop(0)\n",
    "        action = follower.get_next_action(goal_point)\n",
    "    sim.step(action)\n",
    "\n",
    "last_w = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=demo/embedding_traj_from_simulator.gif>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imageio.mimwrite('demo/embedding_traj_from_simulator.gif', imgs, fps=15)\n",
    "display(HTML('<img src={}>'.format(\"demo/embedding_traj_from_simulator.gif\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore inside RNR-Map\n",
    "- Press 'w, a, s, d' to move\n",
    "- Press 'q' to quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "Rt_current = torch.eye(4).unsqueeze(0).to(device).unsqueeze(1)\n",
    "while True:\n",
    "    with torch.no_grad():\n",
    "        rgb, _ = embedder.generate(last_w, {\"Rt\": Rt_current, 'K': K.unsqueeze(1)}, out_res=64)\n",
    "        rgb = (rgb.squeeze().permute(1,2,0).detach().cpu() * 255).numpy().astype(np.uint8)\n",
    "        rgb = cv2.resize(rgb, (obs['rgb'][0].shape[0], obs['rgb'][0].shape[0]))\n",
    "        rgb = add_title(rgb, 'Recon Image')\n",
    "        w_color = copy.deepcopy(last_w_im)\n",
    "        w_color = add_agent_view_on_w(w_color, Rt_current, embedder.coordinate_scale, embedder.w_size, agent_size=4, view_size=15)\n",
    "        w_color = np.fliplr(w_color)\n",
    "        w_color = add_title(w_color, 'Map')\n",
    "\n",
    "        sim_Rt = np.linalg.inv(Rt_current.squeeze().detach().cpu().numpy() @ orig_Rt)\n",
    "        position = sim_Rt[:3,3]\n",
    "        rotation = sim_Rt[:3,:3]\n",
    "        gt_obs = sim.get_observations_at(position, q.from_rotation_matrix(rotation))\n",
    "        gt_rgb = add_title(gt_obs['rgb'], 'GT Image')\n",
    "\n",
    "        view_im = np.concatenate([gt_rgb, rgb, w_color],1)\n",
    "        cv2.imshow(\"view\", view_im[:,:,::-1])\n",
    "        key = cv2.waitKey(0)\n",
    "        if key == ord('q'): break\n",
    "        elif key == ord('a'):\n",
    "            Rt = rotate_n(n=-10.0).to(device)\n",
    "            Rt_current = (Rt@Rt_current.squeeze()).unsqueeze(0).unsqueeze(0)\n",
    "        elif key == ord('d'):\n",
    "            Rt = rotate_n(n=10.0).to(device)\n",
    "            Rt_current = (Rt@Rt_current.squeeze()).unsqueeze(0).unsqueeze(0)\n",
    "        elif key == ord(\"w\"):\n",
    "            Rt_current = go_forward(Rt_current, step=0.1)\n",
    "        elif key == ord('s'):\n",
    "            Rt_current = go_backward(Rt_current, step=0.1)\n",
    "        images.append(view_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=demo/explore_RNR_map_from_simluator.gif>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imageio.mimwrite(\"demo/explore_RNR_map_from_simluator.gif\", images, fps=15)\n",
    "display(HTML('<img src={}>'.format(\"demo/explore_RNR_map_from_simluator.gif\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnr",
   "language": "python",
   "name": "rnr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
